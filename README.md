# XHDoubanCrawler
---
这是一个简单的豆瓣爬虫，目的是爬取豆瓣电影页面，~~实际目的是初学 NodeJS 上手~~，之后对页面进行分析处理抓取想要的结果。  
## Dependencies
```
"dependencies": {
  "cheerio": "~0.20.0",
  "request": "~2.69.0",
  "pinyin": "~2.7.1",
  "heap": "~0.2.6",
  "mkdirp": "~0.5.1"
}
```  
*	**request**: 网络请求库
*	**cheerio**: 服务器端的 JQuery
*	**heap**: javascript 堆实现
*	**pinyin**: 汉字拼音处理(排序用)
*	**mkdirp**: node.js 新建文件夹模块

## Environment
```
"engines": {
  "node": "0.10.x",
  "npm": "1.3.x"
}
```   
## Application
这个爬虫的爬取策略是，我先选出一组若干个种子 URL 进行爬取，过程中分析每个 URL 的 DOM 结构中的「喜欢这部电影的人也喜欢」列表，再递归爬取。其中关于对 DOM 的处理以及其他页面分析行为，比如排除无效页面，排除电视剧页面等问题请详见代码。另外，这个爬虫还对断点或者网络出现其他情况，比如被服务器`403`禁止访问等进行了优化，记录了断点时刻的各变量，支持从断点时刻重启爬虫(~~针对`403`问题， 我的对策是更换 VPN~~)。所有版本支持某些自定义设置，请在代码头进行调整，比如抓取要求，抓取数目，影片排列顺序(按名字，年份，导演，评分)等等。  
### v0.0.1
这个版本是最初写的，其中包括 `app_dfs.js` 和 `app_bfs.js`，分别是采取了深度优先和宽度优先的策略来实现爬取功能。这个版本没有将爬取和页面分析以及结果输出分开，三个部分放在了一起，比较混乱。  
### v0.0.2  
这个版本包括 `app_crawl_dfs.js` 和 `app_crawl_bfs.js`，分别是采取了深度优先和宽度优先的策略来实现爬取功能。这个版本将爬取和页面分析功能剥离开来，`app_parse.js` 功能为结果输出。被爬取的有效(满足要求，比如评分大于8分)页面被存储在本地文件夹中。  
### v0.0.3  
这个版本将以上提到的三个功能完全分开来，其中 `app_crawl_dfs.js` 和 `app_crawl_bfs.js` 只负责爬取足够多的有效电影页面并保存到本地，这个功能不需要输入(断点重启时会自动检测本地存储的数据)，其输出是一个包含全部有效页面源码的文件夹。而 `app_parse.js` 功能是对这些页面进行自定义分析，比如挑选出大于某个评分的电影或是其他自定义规则，这个功能的输入时上一步输出的全部页面源码文件，输出是一个 JSON 对象的序列化文件。最后 `app_output.js` 将上一步的 JSON 对象序列化文件作为输入对所有结果按照自定义规则进行排序并输出 `.txt` 和 `.md` 的结果。这样一来，爬取可以做到最有效率，分析则可以满足更多自定义要求，输出也更快更准确高效。  
## Results
我把一部分的爬取结果放到了这个仓库，请下载仓库并在 `#results` 中查看。 另外，我将2000部8分以上的电影进行了处理，放到了[这个页面](http://xiaohuang.rocks/xhdoubancrawler/)，其中有一个随机推荐功能，欢迎点击:  

<a href="http://xiaohuang.rocks/xhdoubancrawler/"><img src="./xhdoubancrawler-gh-pages/sources/bg.jpg" width="100%"></a>

